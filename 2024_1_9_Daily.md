# 2024年1月9日 日报

今天的工作内容是通过继续阅读[LLM Visualization](https://bbycroft.net/)，结合论文理解gpt的底层原理

- Embeddings，这个部分把输入进去的文本向量化，向量化以后每个单词（token）会或的在一个多维空间的位置信息，这个位置信息代表了这个token的语义，语义相近的两个词在向量空间的距离也会比较近，然后存在一个位置编码，位置编码用于保存每个词在句子中间的位置，有了位置编码llm才可以理解语序，让向量和位置编码点乘后会得到一堆计算结果，这个过程就叫Embeddings
- Layer Norm，对Embeddings的结果进行归一化处理，均值为0，方差为1换句话说就是转化成正态分布，在Layer Norm做这个正态分布转化计算的过程中还加上了权重和偏移量的存在，就是说Layer Norm是一个稳定训练过程的技术，用于让向前传播更加稳定
- 自注意力Self-Attention，这个是Transformer架构的特点，输入部分包含三块，q表示当前位置信息，k表示用于查询比较，v用于储存信息，自注意力层的输出就是这三个向量通过加权计算得到的，接下来还无通过很多个自注意力层，每个自注意力层关注不同的方面
- 在编码器的最后，会传递给解码器作为初始状态，在这边会经过一个线性变化，输入就是编码器中传出的注意力信息，输出是词汇表中每一个单词的得分，也就是Logits
- 引入softmax函数，输入上面的得分，将这些得分转换为概率分布，概率分布选择了这句话下一个最有可能出现的单词

明天的工作等待安排
